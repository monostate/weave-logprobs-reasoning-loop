{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "906d4acb",
   "metadata": {},
   "source": [
    "# Weave + OpenAI Responses API: uncertainty-aware generation\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "- Call the OpenAI Responses API with `include=[\"message.output_text.logprobs\"]` and `top_logprobs`.\n",
    "- Compute token-level uncertainty (perplexity) from logprobs.\n",
    "- If uncertainty is high, run a refinement pass that informs the model about uncertain regions and top-k alternatives.\n",
    "- Log all inputs, outputs, and metrics to Weave using `@weave.op` so you can inspect traces and compare iterations.\n",
    "\n",
    "Prereqs: set `OPENAI_API_KEY` in your environment and install `weave` and `openai`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488a8eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install weave openai set-env-colab-kaggle-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad43363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set your OpenAI API key\n",
    "from set_env import set_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca740a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your OPENAI_API_KEY in the secrets panel to the left üóùÔ∏è\n",
    "_ = set_env(\"OPENAI_API_KEY\")\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" # alternatively, put your key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea04410",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"weave-intro-notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75452a4",
   "metadata": {},
   "source": [
    "## What we'll build\n",
    "\n",
    "- A single `@weave.op` function that implements the uncertainty-aware loop with the Responses API.\n",
    "- The op returns structured metrics (average logprob, perplexity, whether refinement ran) and the final answer.\n",
    "- You can iterate on thresholds, `top_logprobs`, or prompts and compare runs in Weave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00329b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da2eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead37d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.init(PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefce74a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8c24ac",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def _extract_text_and_logprobs(resp):\n",
    "    \"\"\"Extract output text, per-token logprobs, and top-k alternatives from a Responses API result.\"\"\"\n",
    "    # Try to get a plain dict\n",
    "    try:\n",
    "        data = resp.model_dump()\n",
    "    except Exception:\n",
    "        try:\n",
    "            data = json.loads(resp.json())\n",
    "        except Exception:\n",
    "            data = {}\n",
    "\n",
    "    text = getattr(resp, \"output_text\", None) or \"\"\n",
    "    token_logprobs = []\n",
    "    topk_by_pos = []\n",
    "\n",
    "    outputs = data.get(\"output\") or data.get(\"outputs\") or []\n",
    "    if isinstance(outputs, list):\n",
    "        for item in outputs:\n",
    "            if not isinstance(item, dict) or item.get(\"type\") != \"message\":\n",
    "                continue\n",
    "            for content in item.get(\"content\", []) or []:\n",
    "                if not isinstance(content, dict):\n",
    "                    continue\n",
    "                logprobs_payload = None\n",
    "                if \"output_text\" in content and isinstance(content[\"output_text\"], dict):\n",
    "                    inner = content[\"output_text\"]\n",
    "                    text = inner.get(\"text\", text)\n",
    "                    logprobs_payload = inner.get(\"logprobs\")\n",
    "                else:\n",
    "                    if \"text\" in content:\n",
    "                        text = content.get(\"text\", text)\n",
    "                    logprobs_payload = content.get(\"logprobs\")\n",
    "\n",
    "                if isinstance(logprobs_payload, dict):\n",
    "                    raw_lps = logprobs_payload.get(\"token_logprobs\") or []\n",
    "                    if isinstance(raw_lps, list):\n",
    "                        token_logprobs = [float(x) for x in raw_lps if x is not None]\n",
    "                    raw_top = logprobs_payload.get(\"top_logprobs\")\n",
    "                    if isinstance(raw_top, list):\n",
    "                        parsed = []\n",
    "                        for pos in raw_top:\n",
    "                            alts = []\n",
    "                            if isinstance(pos, list):\n",
    "                                for alt in pos:\n",
    "                                    if isinstance(alt, dict):\n",
    "                                        tok = alt.get(\"token\") or alt.get(\"text\") or \"\"\n",
    "                                        lp = alt.get(\"logprob\")\n",
    "                                        if lp is not None:\n",
    "                                            alts.append((str(tok), float(lp)))\n",
    "                            parsed.append(alts)\n",
    "                        topk_by_pos = parsed\n",
    "\n",
    "    return text, token_logprobs, topk_by_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a21db22",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def _perplexity(token_logprobs):\n",
    "    if not token_logprobs:\n",
    "        return float(\"inf\")\n",
    "    return math.exp(-sum(token_logprobs) / max(1, len(token_logprobs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3401078f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def _uncertainty_report(token_logprobs, topk_by_pos, max_positions=5):\n",
    "    if not token_logprobs:\n",
    "        return \"No token-level logprob info available.\"\n",
    "    indices = list(range(len(token_logprobs)))\n",
    "    indices.sort(key=lambda i: token_logprobs[i])\n",
    "    lines = []\n",
    "    for idx in indices[:max_positions]:\n",
    "        alts = topk_by_pos[idx] if idx < len(topk_by_pos) else []\n",
    "        formatted = \", \".join(f\"{tok} ({lp:.2f})\" for tok, lp in alts)\n",
    "        lines.append(f\"pos {idx}: lp={token_logprobs[idx]:.2f}; alts: {formatted}\")\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb13134",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def _uncertainty_table(token_logprobs, topk_by_pos, max_positions=5):\n",
    "    \"\"\"Return a compact table of the most uncertain positions and their top-k alternatives.\n",
    "\n",
    "    Shape: [{\"position\": int, \"token_logprob\": float, \"top_k\": [{\"token\": str, \"logprob\": float}]}]\n",
    "    \"\"\"\n",
    "    if not token_logprobs:\n",
    "        return []\n",
    "    indices = list(range(len(token_logprobs)))\n",
    "    indices.sort(key=lambda i: token_logprobs[i])\n",
    "    rows = []\n",
    "    for idx in indices[:max_positions]:\n",
    "        alts = []\n",
    "        if idx < len(topk_by_pos):\n",
    "            for alt in (topk_by_pos[idx] or []):\n",
    "                if isinstance(alt, tuple):\n",
    "                    tok, lp = alt\n",
    "                elif isinstance(alt, dict):\n",
    "                    tok, lp = alt.get(\"token\"), alt.get(\"logprob\")\n",
    "                else:\n",
    "                    tok, lp = None, None\n",
    "                if tok is not None and lp is not None:\n",
    "                    alts.append({\"token\": str(tok), \"logprob\": float(lp)})\n",
    "        rows.append(\n",
    "            {\n",
    "                \"position\": idx,\n",
    "                \"token_logprob\": float(token_logprobs[idx]),\n",
    "                \"top_k\": alts,\n",
    "            }\n",
    "        )\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb934401",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Simple cost table (USD) per 1M tokens\n",
    "PRICING_USD_PER_MILLION = {\n",
    "    \"gpt-4.1-mini\": {\"input\": 0.40, \"output\": 1.60},\n",
    "    \"o4-mini\": {\"input\": 1.10, \"output\": 4.40},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f631027",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def _extract_usage(resp):\n",
    "    \"\"\"Return (input_tokens, output_tokens) if present, else (None, None).\"\"\"\n",
    "    try:\n",
    "        data = resp.model_dump()\n",
    "    except Exception:\n",
    "        try:\n",
    "            data = json.loads(resp.json())\n",
    "        except Exception:\n",
    "            data = {}\n",
    "\n",
    "    usage = data.get(\"usage\") or {}\n",
    "    # The Responses API commonly provides input_tokens/output_tokens\n",
    "    in_tok = usage.get(\"input_tokens\")\n",
    "    out_tok = usage.get(\"output_tokens\")\n",
    "    if isinstance(in_tok, int) and isinstance(out_tok, int):\n",
    "        return in_tok, out_tok\n",
    "    # Fallbacks if the shape differs\n",
    "    in_tok = usage.get(\"prompt_tokens\") or usage.get(\"input_tokens_total\")\n",
    "    out_tok = usage.get(\"completion_tokens\") or usage.get(\"output_tokens_total\")\n",
    "    if isinstance(in_tok, int) and isinstance(out_tok, int):\n",
    "        return in_tok, out_tok\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c8e01f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def _estimate_cost_usd(model: str, input_tokens: int | None, output_tokens: int | None) -> float | None:\n",
    "    pricing = PRICING_USD_PER_MILLION.get(model)\n",
    "    if not pricing or input_tokens is None or output_tokens is None:\n",
    "        return None\n",
    "    return (input_tokens / 1_000_000.0) * pricing[\"input\"] + (output_tokens / 1_000_000.0) * pricing[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3259fa8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def _extract_reasoning_metadata(resp):\n",
    "    \"\"\"Extract basic reasoning metadata if present: count and encrypted length.\n",
    "\n",
    "    Returns dict(reasoning_items: int | None, encrypted_chars: int | None)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = resp.model_dump()\n",
    "    except Exception:\n",
    "        try:\n",
    "            data = json.loads(resp.json())\n",
    "        except Exception:\n",
    "            data = {}\n",
    "\n",
    "    outputs = data.get(\"output\") or data.get(\"outputs\") or []\n",
    "    if not isinstance(outputs, list):\n",
    "        return {\"reasoning_items\": None, \"encrypted_chars\": None}\n",
    "\n",
    "    reasoning_items = 0\n",
    "    encrypted_chars = 0\n",
    "    for item in outputs:\n",
    "        if not isinstance(item, dict):\n",
    "            continue\n",
    "        if item.get(\"type\") != \"reasoning\":\n",
    "            continue\n",
    "        reasoning_items += 1\n",
    "        for content in item.get(\"content\", []) or []:\n",
    "            if isinstance(content, dict):\n",
    "                enc = content.get(\"encrypted_content\")\n",
    "                if isinstance(enc, str):\n",
    "                    encrypted_chars += len(enc)\n",
    "\n",
    "    return {\"reasoning_items\": reasoning_items, \"encrypted_chars\": encrypted_chars}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc43ff",
   "metadata": {},
   "source": [
    "## Weave-logged uncertainty loop\n",
    "\n",
    "In this section we wrap the uncertainty-aware generation into a single `@weave.op`. Weave will:\n",
    "- Log function code, inputs (question and parameters), and outputs.\n",
    "- Capture nested OpenAI calls (first pass and optional refinement).\n",
    "- Let you inspect token logprobs, perplexity, and compare experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f902ca63",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def answer_difficult_question_with_uncertainty(\n",
    "    question: str,\n",
    "    model: str = \"gpt-4.1-mini\",\n",
    "    top_k: int = 5,\n",
    "    threshold: float = 1.4,\n",
    "    temperature: float = 0.2,\n",
    "):\n",
    "    t0 = time.perf_counter()\n",
    "    resp1 = client.responses.create(\n",
    "        model=model,\n",
    "        instructions=\"You are a precise cryptography expert. Be concise and accurate.\",\n",
    "        input=question,\n",
    "        temperature=temperature,\n",
    "        top_logprobs=top_k,\n",
    "        include=[\"message.output_text.logprobs\"],\n",
    "    )\n",
    "\n",
    "    text1, token_lps1, topk1 = _extract_text_and_logprobs(resp1)\n",
    "    in_tok1, out_tok1 = _extract_usage(resp1)\n",
    "    ppx1 = _perplexity(token_lps1)\n",
    "    avg_lp1 = (sum(token_lps1) / len(token_lps1)) if token_lps1 else None\n",
    "    table1 = _uncertainty_table(token_lps1, topk1, max_positions=5)\n",
    "\n",
    "    did_refine = False\n",
    "    final_text = text1\n",
    "    ppx2 = None\n",
    "    avg_lp2 = None\n",
    "\n",
    "    if ppx1 > threshold:\n",
    "        did_refine = True\n",
    "        analysis = _uncertainty_report(token_lps1, topk1, max_positions=5)\n",
    "        refined_input = (\n",
    "            \"You previously drafted an answer to a difficult question.\\n\"\n",
    "            f\"Question: {question}\\n\\n\"\n",
    "            f\"Your draft answer:\\n{text1}\\n\\n\"\n",
    "            f\"Uncertainty analysis (perplexity={ppx1:.3f}):\\n{analysis}\\n\\n\"\n",
    "            \"Revise the answer to improve factual accuracy and clarity, resolving uncertain parts. \"\n",
    "            \"Do not mention this analysis in the final answer.\"\n",
    "        )\n",
    "\n",
    "        resp2 = client.responses.create(\n",
    "            model=model,\n",
    "            instructions=\"You are a precise cryptography expert. Be concise and accurate.\",\n",
    "            input=refined_input,\n",
    "            temperature=max(0.0, temperature - 0.1),\n",
    "            top_logprobs=top_k,\n",
    "            include=[\"message.output_text.logprobs\"],\n",
    "        )\n",
    "\n",
    "        text2, token_lps2, _ = _extract_text_and_logprobs(resp2)\n",
    "        in_tok2, out_tok2 = _extract_usage(resp2)\n",
    "        ppx2 = _perplexity(token_lps2)\n",
    "        avg_lp2 = (sum(token_lps2) / len(token_lps2)) if token_lps2 else None\n",
    "        final_text = text2\n",
    "        table2 = _uncertainty_table(token_lps2, topk1, max_positions=5)\n",
    "    else:\n",
    "        table2 = None\n",
    "        in_tok2, out_tok2 = None, None\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    # Attach a concise summary to the Weave call for easy inspection in the UI\n",
    "    try:\n",
    "        current_call = weave.require_current_call()\n",
    "        current_call.summary = {\n",
    "            \"parameters\": {\n",
    "                \"model\": model,\n",
    "                \"top_k\": top_k,\n",
    "                \"threshold\": threshold,\n",
    "                \"temperature\": temperature,\n",
    "            },\n",
    "            \"first_pass\": {\n",
    "                \"token_count\": len(token_lps1),\n",
    "                \"avg_logprob\": avg_lp1,\n",
    "                \"perplexity\": ppx1,\n",
    "            },\n",
    "            \"refinement\": {\n",
    "                \"enabled\": did_refine,\n",
    "                \"avg_logprob_after\": avg_lp2,\n",
    "                \"perplexity_after\": ppx2,\n",
    "            },\n",
    "            \"timing_seconds\": t1 - t0,\n",
    "        }\n",
    "    except Exception:\n",
    "        # If not in a tracked context, just skip summary attachment\n",
    "        pass\n",
    "\n",
    "    # Aggregate token usage/costs\n",
    "    total_input_tokens = (in_tok1 or 0) + (in_tok2 or 0)\n",
    "    total_output_tokens = (out_tok1 or 0) + (out_tok2 or 0)\n",
    "    estimated_cost_usd = _estimate_cost_usd(model, total_input_tokens, total_output_tokens)\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"final_answer\": final_text,\n",
    "        \"first_pass\": {\n",
    "            \"answer\": text1,\n",
    "            \"avg_logprob\": avg_lp1,\n",
    "            \"perplexity\": ppx1,\n",
    "            \"uncertainty_table\": table1,\n",
    "            \"input_tokens\": in_tok1,\n",
    "            \"output_tokens\": out_tok1,\n",
    "        },\n",
    "        \"refinement\": {\n",
    "            \"enabled\": did_refine,\n",
    "            \"perplexity_after\": ppx2,\n",
    "            \"avg_logprob_after\": avg_lp2,\n",
    "            \"uncertainty_table_after\": table2,\n",
    "            \"input_tokens_after\": in_tok2,\n",
    "            \"output_tokens_after\": out_tok2,\n",
    "        },\n",
    "        \"parameters\": {\n",
    "            \"model\": model,\n",
    "            \"top_k\": top_k,\n",
    "            \"threshold\": threshold,\n",
    "            \"temperature\": temperature,\n",
    "        },\n",
    "        \"usage\": {\n",
    "            \"total_input_tokens\": total_input_tokens if (in_tok1 is not None or in_tok2 is not None) else None,\n",
    "            \"total_output_tokens\": total_output_tokens if (out_tok1 is not None or out_tok2 is not None) else None,\n",
    "            \"estimated_cost_usd\": estimated_cost_usd,\n",
    "            \"timing_seconds\": t1 - t0,\n",
    "        },\n",
    "        \"model_kind\": \"non_reasoning\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b876a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a difficult question and log everything to Weave via the op above\n",
    "question = (\n",
    "    \"What are the implications of P vs NP for modern cryptography? Provide concrete examples and caveats.\"\n",
    ")\n",
    "with weave.attributes({\n",
    "    \"tag\": \"uncertainty-loop\",\n",
    "    \"question\": question,\n",
    "    \"question_topic\": \"cryptography\",\n",
    "    \"variant\": \"gpt-4.1-mini\",\n",
    "}):\n",
    "    base_result = answer_difficult_question_with_uncertainty(\n",
    "        question,\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        top_k=5,\n",
    "        threshold=1.4,\n",
    "        temperature=0.2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68825a40",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "with weave.attributes({\n",
    "    \"tag\": \"uncertainty-loop\",\n",
    "    \"question\": question,\n",
    "    \"question_topic\": \"cryptography\",\n",
    "    \"variant\": \"o4-mini\",\n",
    "}):\n",
    "    reasoning_result = answer_difficult_question_with_uncertainty(\n",
    "        question,\n",
    "        model=\"o4-mini\",\n",
    "        top_k=5,\n",
    "        threshold=1.4,\n",
    "        temperature=0.2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6932ba69",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def _fmt_cost(x):\n",
    "    return f\"${x:.4f}\" if isinstance(x, (int, float)) and x is not None else \"-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf059be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n==== Final Answers ====\")\n",
    "print(\"[gpt-4.1-mini]\\n\", base_result.get(\"final_answer\", \"\"))\n",
    "print(\"\\n[o4-mini]\\n\", reasoning_result.get(\"final_answer\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d2f808",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n==== Usage/Cost/Time (estimates) ====\")\n",
    "base_usage = base_result.get(\"usage\", {})\n",
    "reason_usage = reasoning_result.get(\"usage\", {})\n",
    "print(\n",
    "    \"gpt-4.1-mini:\",\n",
    "    \"tokens(in/out)=\",\n",
    "    (base_usage.get(\"total_input_tokens\"), base_usage.get(\"total_output_tokens\")),\n",
    "    \"cost=\",\n",
    "    _fmt_cost(base_usage.get(\"estimated_cost_usd\")),\n",
    "    \"time(s)=\",\n",
    "    base_usage.get(\"timing_seconds\"),\n",
    ")\n",
    "print(\n",
    "    \"o4-mini:\",\n",
    "    \"tokens(in/out)=\",\n",
    "    (reason_usage.get(\"total_input_tokens\"), reason_usage.get(\"total_output_tokens\")),\n",
    "    \"cost=\",\n",
    "    _fmt_cost(reason_usage.get(\"estimated_cost_usd\")),\n",
    "    \"time(s)=\",\n",
    "    reason_usage.get(\"timing_seconds\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74e5cd7",
   "metadata": {},
   "source": [
    "## End\n",
    "\n",
    "This notebook now focuses on the Weave-logged uncertainty-aware generation loop using the OpenAI Responses API.\n",
    "Use the Weave UI links to explore traces, inputs/outputs, and compare iterations."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
