{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57370dee",
   "metadata": {},
   "source": [
    "# Weave + OpenAI Responses API: uncertainty-aware generation\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "- Call the OpenAI Responses API with `include=[\"message.output_text.logprobs\"]` and `top_logprobs`.\n",
    "- Compute token-level uncertainty (perplexity) from logprobs.\n",
    "- If uncertainty is high, run a refinement pass that informs the model about uncertain regions and top-k alternatives.\n",
    "- Log all inputs, outputs, and metrics to Weave using `@weave.op` so you can inspect traces and compare iterations.\n",
    "\n",
    "Prereqs: set `OPENAI_API_KEY` in your environment and install `weave` and `openai`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421724db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies - handles both local and cloud environments\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if we're in a local environment with vendorized polyfile-weave\n",
    "local_polyfile = Path(\"./polyfile-weave\")\n",
    "if local_polyfile.exists() and local_polyfile.is_dir():\n",
    "    print(\"Found local polyfile-weave, installing from vendorized source...\")\n",
    "    # Install local polyfile-weave first (with fixes for Python 3.9+ compatibility)\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", \"./polyfile-weave\"])\n",
    "    print(\"✓ Installed local polyfile-weave\")\n",
    "\n",
    "# Install remaining dependencies\n",
    "try:\n",
    "    import weave\n",
    "    import openai\n",
    "    import packaging\n",
    "    print(\"✓ Required packages already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing required packages...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"weave\", \"openai\", \"packaging\", \"gql>=4.0.0\", \"set-env-colab-kaggle-dotenv\"])\n",
    "    print(\"✓ Installed required packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea00731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Try to load from .env file if it exists\n",
    "env_file = Path(\".env\")\n",
    "if env_file.exists():\n",
    "    with open(env_file) as f:\n",
    "        for line in f:\n",
    "            if line.strip() and not line.startswith(\"#\"):\n",
    "                key, value = line.strip().split(\"=\", 1)\n",
    "                os.environ[key] = value\n",
    "\n",
    "# For notebooks, try set_env if available\n",
    "try:\n",
    "    from set_env import set_env\n",
    "    _ = set_env(\"OPENAI_API_KEY\")\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a9d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure OPENAI_API_KEY is set\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"Warning: OPENAI_API_KEY not set. Please set it in your environment or .env file\")\n",
    "    print(\"Example: export OPENAI_API_KEY='sk-...'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8ce4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = os.environ.get(\"WEAVE_PROJECT\", \"weave-intro-notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d244b0",
   "metadata": {},
   "source": [
    "## What we'll build\n",
    "\n",
    "- A single `@weave.op` function that implements the uncertainty-aware loop with the Responses API.\n",
    "- The op returns structured metrics (average logprob, perplexity, whether refinement ran) and the final answer.\n",
    "- You can iterate on thresholds, `top_logprobs`, or prompts and compare runs in Weave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb2f528",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e7c0bf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import weave\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fe24f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply runtime patch for gql 4.x compatibility by patching gql.Client directly\n",
    "def patch_gql_client_for_v4():\n",
    "    \"\"\"\n",
    "    Monkey-patch gql.Client.execute to handle the signature change between v3 and v4.\n",
    "    This is a more direct approach that doesn't require modifying Weave's internal structure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import gql\n",
    "        from gql import Client\n",
    "        from packaging import version\n",
    "        \n",
    "        # Check gql version\n",
    "        GQL_VERSION = version.parse(gql.__version__ if hasattr(gql, '__version__') else '3.0.0')\n",
    "        GQL_V4_PLUS = GQL_VERSION >= version.parse('4.0.0')\n",
    "        \n",
    "        print(f\"Detected gql version: {GQL_VERSION}\")\n",
    "        \n",
    "        if not GQL_V4_PLUS:\n",
    "            print(\"gql 3.x detected, no patch needed\")\n",
    "            return True\n",
    "        \n",
    "        # Store original execute methods\n",
    "        from gql.client import SyncClientSession, AsyncClientSession\n",
    "        \n",
    "        _orig_sync_execute = SyncClientSession.execute\n",
    "        _orig_async_execute = AsyncClientSession.execute\n",
    "        \n",
    "        # Create wrapper that handles both call signatures\n",
    "        def patched_sync_execute(self, document, *args, **kwargs):\n",
    "            \"\"\"Wrapper that accepts both v3 and v4 call signatures\"\"\"\n",
    "            # If called with positional args (v3 style), convert to v4 style\n",
    "            if args and 'variable_values' not in kwargs:\n",
    "                # v3 style: execute(query, variables)\n",
    "                kwargs['variable_values'] = args[0]\n",
    "                return _orig_sync_execute(self, document, **kwargs)\n",
    "            else:\n",
    "                # v4 style or no variables\n",
    "                return _orig_sync_execute(self, document, *args, **kwargs)\n",
    "        \n",
    "        async def patched_async_execute(self, document, *args, **kwargs):\n",
    "            \"\"\"Async wrapper that accepts both v3 and v4 call signatures\"\"\"\n",
    "            # If called with positional args (v3 style), convert to v4 style\n",
    "            if args and 'variable_values' not in kwargs:\n",
    "                # v3 style: execute(query, variables)\n",
    "                kwargs['variable_values'] = args[0]\n",
    "                return await _orig_async_execute(self, document, **kwargs)\n",
    "            else:\n",
    "                # v4 style or no variables\n",
    "                return await _orig_async_execute(self, document, *args, **kwargs)\n",
    "        \n",
    "        # Apply patches\n",
    "        SyncClientSession.execute = patched_sync_execute\n",
    "        AsyncClientSession.execute = patched_async_execute\n",
    "        \n",
    "        print(\"✓ Patched gql.Client for v3/v4 compatibility\")\n",
    "        return True\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"ERROR: Could not import gql modules: {e}\")\n",
    "        raise RuntimeError(f\"Failed to apply gql patch: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to patch gql.Client: {e}\")\n",
    "        raise RuntimeError(f\"Failed to apply gql patch: {e}\")\n",
    "\n",
    "# Apply the patch BEFORE initializing Weave\n",
    "patch_gql_client_for_v4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0907aace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weave - REQUIRED for tracking\n",
    "weave.init(PROJECT)\n",
    "print(f\"✓ Weave initialized with project: {PROJECT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d127a31b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90280a61",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def _extract_text_and_logprobs(resp):\n",
    "    \"\"\"Extract output text, per-token logprobs, and top-k alternatives from a Responses API result.\"\"\"\n",
    "    # Try to get a plain dict\n",
    "    try:\n",
    "        data = resp.model_dump()\n",
    "    except Exception:\n",
    "        try:\n",
    "            data = json.loads(resp.json())\n",
    "        except Exception:\n",
    "            data = {}\n",
    "\n",
    "    text = getattr(resp, \"output_text\", None) or \"\"\n",
    "    token_logprobs = []\n",
    "    topk_by_pos = []\n",
    "    tokens = []  # Store actual token text\n",
    "\n",
    "    outputs = data.get(\"output\") or data.get(\"outputs\") or []\n",
    "    if isinstance(outputs, list):\n",
    "        for item in outputs:\n",
    "            if not isinstance(item, dict) or item.get(\"type\") != \"message\":\n",
    "                continue\n",
    "            for content in item.get(\"content\", []) or []:\n",
    "                if not isinstance(content, dict):\n",
    "                    continue\n",
    "                \n",
    "                # Extract text\n",
    "                if \"text\" in content:\n",
    "                    text = content.get(\"text\", text)\n",
    "                \n",
    "                # NEW: Handle the actual API format - logprobs is a LIST, not a dict!\n",
    "                logprobs_list = content.get(\"logprobs\")\n",
    "                if isinstance(logprobs_list, list):\n",
    "                    # Each item has: token, logprob, top_logprobs\n",
    "                    for token_data in logprobs_list:\n",
    "                        if isinstance(token_data, dict):\n",
    "                            # Extract this token's logprob\n",
    "                            token_text = token_data.get(\"token\", \"\")\n",
    "                            token_lp = token_data.get(\"logprob\")\n",
    "                            if token_lp is not None:\n",
    "                                token_logprobs.append(float(token_lp))\n",
    "                                tokens.append(token_text)\n",
    "                            \n",
    "                            # Extract top-k alternatives for this position\n",
    "                            top_alts = token_data.get(\"top_logprobs\", [])\n",
    "                            alts = []\n",
    "                            if isinstance(top_alts, list):\n",
    "                                for alt in top_alts:\n",
    "                                    if isinstance(alt, dict):\n",
    "                                        alt_token = alt.get(\"token\", \"\")\n",
    "                                        alt_lp = alt.get(\"logprob\")\n",
    "                                        if alt_lp is not None:\n",
    "                                            alts.append((alt_token, float(alt_lp)))\n",
    "                            topk_by_pos.append(alts)\n",
    "\n",
    "    # Return tokens as well for better analysis\n",
    "    return text, token_logprobs, topk_by_pos, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55f0e2d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def _perplexity(token_logprobs):\n",
    "    if not token_logprobs:\n",
    "        # No logprobs means we can't calculate perplexity - don't trigger refinement\n",
    "        return 0.0  # Low perplexity = high confidence\n",
    "    if len(token_logprobs) == 0:\n",
    "        return 0.0\n",
    "    avg_logprob = sum(token_logprobs) / len(token_logprobs)\n",
    "    # Perplexity = exp(-avg_logprob)\n",
    "    # Lower perplexity = more confident\n",
    "    # Higher perplexity = more uncertain\n",
    "    return math.exp(-avg_logprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85fb9f8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def _calculate_entropy(top_k_alternatives):\n",
    "    \"\"\"Calculate entropy from top-k alternatives at a position.\n",
    "    Higher entropy = more uncertainty about which token to choose.\n",
    "    \"\"\"\n",
    "    if not top_k_alternatives:\n",
    "        return 0.0\n",
    "    \n",
    "    # Convert logprobs to probabilities\n",
    "    probs = []\n",
    "    for token, logprob in top_k_alternatives:\n",
    "        probs.append(math.exp(logprob))\n",
    "    \n",
    "    # Normalize (they should sum to ~1 already for top token)\n",
    "    total = sum(probs)\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    probs = [p/total for p in probs]\n",
    "    \n",
    "    # Calculate entropy: -sum(p * log(p))\n",
    "    entropy = 0.0\n",
    "    for p in probs:\n",
    "        if p > 0:\n",
    "            entropy -= p * math.log(p)\n",
    "    return entropy\n",
    "\n",
    "def _uncertainty_report(token_logprobs, topk_by_pos, tokens=None, max_positions=10):\n",
    "    if not token_logprobs:\n",
    "        return \"No token-level logprob info available.\"\n",
    "    \n",
    "    # Calculate entropy for each position\n",
    "    entropies = []\n",
    "    for i, alts in enumerate(topk_by_pos):\n",
    "        if i < len(token_logprobs):\n",
    "            entropies.append(_calculate_entropy(alts))\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_entropy = sum(entropies) / len(entropies) if entropies else 0\n",
    "    max_entropy = max(entropies) if entropies else 0\n",
    "    low_confidence_count = sum(1 for lp in token_logprobs if math.exp(lp) < 0.5)\n",
    "    very_low_confidence_count = sum(1 for lp in token_logprobs if math.exp(lp) < 0.2)\n",
    "    \n",
    "    # Find most uncertain positions\n",
    "    indices = list(range(len(token_logprobs)))\n",
    "    # Sort by LOWEST logprob (most uncertain)\n",
    "    indices.sort(key=lambda i: token_logprobs[i])\n",
    "    \n",
    "    lines = []\n",
    "    lines.append(\"=== UNCERTAINTY ANALYSIS ===\")\n",
    "    lines.append(f\"Total tokens: {len(token_logprobs)}\")\n",
    "    lines.append(f\"Average entropy: {avg_entropy:.2f}\")\n",
    "    lines.append(f\"Maximum entropy: {max_entropy:.2f}\")\n",
    "    lines.append(f\"Low confidence tokens (<50%): {low_confidence_count}\")\n",
    "    lines.append(f\"Very low confidence tokens (<20%): {very_low_confidence_count}\")\n",
    "    lines.append(\"\")\n",
    "    \n",
    "    # Group uncertain tokens with context\n",
    "    lines.append(f\"Most uncertain tokens (top {min(max_positions, len(indices))}):\")\n",
    "    for rank, idx in enumerate(indices[:max_positions], 1):\n",
    "        token_text = tokens[idx] if tokens and idx < len(tokens) else f\"[pos {idx}]\"\n",
    "        lp = token_logprobs[idx]\n",
    "        prob = math.exp(lp) * 100  # Convert to percentage\n",
    "        entropy = entropies[idx] if idx < len(entropies) else 0.0\n",
    "        \n",
    "        # Get surrounding context (2 tokens before and after)\n",
    "        context_before = \"\"\n",
    "        context_after = \"\"\n",
    "        if tokens:\n",
    "            start = max(0, idx - 2)\n",
    "            end = min(len(tokens), idx + 3)\n",
    "            context_tokens = tokens[start:end]\n",
    "            context_before = \"\".join(tokens[start:idx])\n",
    "            context_after = \"\".join(tokens[idx+1:end])\n",
    "        \n",
    "        lines.append(f\"\\n  {rank}. Token: '{token_text}' (position {idx})\")\n",
    "        lines.append(f\"     Context: ...{context_before}[{token_text}]{context_after}...\")\n",
    "        lines.append(f\"     Confidence: {prob:.1f}%, Entropy: {entropy:.2f}\")\n",
    "        \n",
    "        alts = topk_by_pos[idx] if idx < len(topk_by_pos) else []\n",
    "        if alts:\n",
    "            top_alts = []\n",
    "            for tok, alt_lp in alts[:5]:  # Show top 5 alternatives\n",
    "                alt_prob = math.exp(alt_lp) * 100\n",
    "                top_alts.append(f\"'{tok}' ({alt_prob:.1f}%)\")\n",
    "            lines.append(f\"     Alternatives: {', '.join(top_alts)}\")\n",
    "    \n",
    "    lines.append(\"\\n=== KEY INSIGHTS ===\")\n",
    "    if very_low_confidence_count > 0:\n",
    "        lines.append(\"- Multiple tokens with very low confidence detected\")\n",
    "    if max_entropy > 2.0:\n",
    "        lines.append(\"- High entropy indicates multiple equally viable options\")\n",
    "    if low_confidence_count > len(token_logprobs) * 0.2:\n",
    "        lines.append(\"- Over 20% of tokens have low confidence\")\n",
    "    \n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d283a2df",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def _uncertainty_table(token_logprobs, topk_by_pos, max_positions=5):\n",
    "    \"\"\"Return a compact table of the most uncertain positions and their top-k alternatives.\n",
    "\n",
    "    Shape: [{\"position\": int, \"token_logprob\": float, \"top_k\": [{\"token\": str, \"logprob\": float}]}]\n",
    "    \"\"\"\n",
    "    if not token_logprobs:\n",
    "        return []\n",
    "    indices = list(range(len(token_logprobs)))\n",
    "    indices.sort(key=lambda i: token_logprobs[i])\n",
    "    rows = []\n",
    "    for idx in indices[:max_positions]:\n",
    "        alts = []\n",
    "        if idx < len(topk_by_pos):\n",
    "            for alt in (topk_by_pos[idx] or []):\n",
    "                if isinstance(alt, tuple):\n",
    "                    tok, lp = alt\n",
    "                elif isinstance(alt, dict):\n",
    "                    tok, lp = alt.get(\"token\"), alt.get(\"logprob\")\n",
    "                else:\n",
    "                    tok, lp = None, None\n",
    "                if tok is not None and lp is not None:\n",
    "                    alts.append({\"token\": str(tok), \"logprob\": float(lp)})\n",
    "        rows.append(\n",
    "            {\n",
    "                \"position\": idx,\n",
    "                \"token_logprob\": float(token_logprobs[idx]),\n",
    "                \"top_k\": alts,\n",
    "            }\n",
    "        )\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17d8d8f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Simple cost table (USD) per 1M tokens\n",
    "PRICING_USD_PER_MILLION = {\n",
    "    \"gpt-4.1-mini\": {\"input\": 0.40, \"output\": 1.60},\n",
    "    \"o4-mini\": {\"input\": 1.10, \"output\": 4.40},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0f47ba",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def _extract_usage(resp):\n",
    "    \"\"\"Return (input_tokens, output_tokens) if present, else (None, None).\"\"\"\n",
    "    try:\n",
    "        data = resp.model_dump()\n",
    "    except Exception:\n",
    "        try:\n",
    "            data = json.loads(resp.json())\n",
    "        except Exception:\n",
    "            data = {}\n",
    "\n",
    "    usage = data.get(\"usage\") or {}\n",
    "    # The Responses API commonly provides input_tokens/output_tokens\n",
    "    in_tok = usage.get(\"input_tokens\")\n",
    "    out_tok = usage.get(\"output_tokens\")\n",
    "    if isinstance(in_tok, int) and isinstance(out_tok, int):\n",
    "        return in_tok, out_tok\n",
    "    # Fallbacks if the shape differs\n",
    "    in_tok = usage.get(\"prompt_tokens\") or usage.get(\"input_tokens_total\")\n",
    "    out_tok = usage.get(\"completion_tokens\") or usage.get(\"output_tokens_total\")\n",
    "    if isinstance(in_tok, int) and isinstance(out_tok, int):\n",
    "        return in_tok, out_tok\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee3ab3b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def _estimate_cost_usd(model: str, input_tokens: int | None, output_tokens: int | None) -> float | None:\n",
    "    pricing = PRICING_USD_PER_MILLION.get(model)\n",
    "    if not pricing or input_tokens is None or output_tokens is None:\n",
    "        return None\n",
    "    return (input_tokens / 1_000_000.0) * pricing[\"input\"] + (output_tokens / 1_000_000.0) * pricing[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9069a847",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def _extract_reasoning_metadata(resp):\n",
    "    \"\"\"Extract basic reasoning metadata if present: count and encrypted length.\n",
    "\n",
    "    Returns dict(reasoning_items: int | None, encrypted_chars: int | None)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = resp.model_dump()\n",
    "    except Exception:\n",
    "        try:\n",
    "            data = json.loads(resp.json())\n",
    "        except Exception:\n",
    "            data = {}\n",
    "\n",
    "    outputs = data.get(\"output\") or data.get(\"outputs\") or []\n",
    "    if not isinstance(outputs, list):\n",
    "        return {\"reasoning_items\": None, \"encrypted_chars\": None}\n",
    "\n",
    "    reasoning_items = 0\n",
    "    encrypted_chars = 0\n",
    "    for item in outputs:\n",
    "        if not isinstance(item, dict):\n",
    "            continue\n",
    "        if item.get(\"type\") != \"reasoning\":\n",
    "            continue\n",
    "        reasoning_items += 1\n",
    "        for content in item.get(\"content\", []) or []:\n",
    "            if isinstance(content, dict):\n",
    "                enc = content.get(\"encrypted_content\")\n",
    "                if isinstance(enc, str):\n",
    "                    encrypted_chars += len(enc)\n",
    "\n",
    "    return {\"reasoning_items\": reasoning_items, \"encrypted_chars\": encrypted_chars}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353892d9",
   "metadata": {},
   "source": [
    "## Weave-logged uncertainty loop\n",
    "\n",
    "In this section we wrap the uncertainty-aware generation into a single `@weave.op`. Weave will:\n",
    "- Log function code, inputs (question and parameters), and outputs.\n",
    "- Capture nested OpenAI calls (first pass and optional refinement).\n",
    "- Let you inspect token logprobs, perplexity, and compare experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfdbd03",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def first_pass_generation(question: str, model: str, temperature: float, top_k: int, is_reasoning_model: bool):\n",
    "    \"\"\"Generate initial response and extract logprobs/uncertainty metrics\"\"\"\n",
    "    t_start = time.perf_counter()\n",
    "    print(f\"  [FIRST PASS] Starting generation with {model}...\")\n",
    "    \n",
    "    create_params = {\n",
    "        \"model\": model,\n",
    "        \"instructions\": \"You are a precise cryptography expert. Be concise and accurate.\",\n",
    "        \"input\": question,\n",
    "    }\n",
    "    \n",
    "    # Only add temperature and logprobs for non-reasoning models\n",
    "    if not is_reasoning_model:\n",
    "        create_params[\"temperature\"] = temperature\n",
    "        create_params[\"top_logprobs\"] = top_k\n",
    "        create_params[\"include\"] = [\"message.output_text.logprobs\"]\n",
    "    \n",
    "    print(f\"  [FIRST PASS] Calling OpenAI API...\")\n",
    "    api_start = time.perf_counter()\n",
    "    resp = client.responses.create(**create_params)\n",
    "    api_end = time.perf_counter()\n",
    "    print(f\"  [FIRST PASS] API call took {api_end - api_start:.2f}s\")\n",
    "    \n",
    "    print(f\"  [FIRST PASS] Extracting metrics...\")\n",
    "    text, token_lps, topk_alts, tokens = _extract_text_and_logprobs(resp)\n",
    "    in_tok, out_tok = _extract_usage(resp)\n",
    "    ppx = _perplexity(token_lps)\n",
    "    avg_lp = (sum(token_lps) / len(token_lps)) if token_lps else None\n",
    "    table = _uncertainty_table(token_lps, topk_alts, max_positions=5)\n",
    "    \n",
    "    t_end = time.perf_counter()\n",
    "    print(f\"  [FIRST PASS] Total time: {t_end - t_start:.2f}s (tokens: {len(token_lps)}, perplexity: {ppx:.2f})\")\n",
    "    \n",
    "    # Log ALL metrics to Weave\n",
    "    return {\n",
    "        \"response\": resp,\n",
    "        \"text\": text,\n",
    "        \"tokens\": tokens,  # NEW: actual token text\n",
    "        \"token_logprobs\": token_lps,\n",
    "        \"top_k_alternatives\": topk_alts,\n",
    "        \"perplexity\": ppx,\n",
    "        \"avg_logprob\": avg_lp,\n",
    "        \"uncertainty_table\": table,\n",
    "        \"input_tokens\": in_tok,\n",
    "        \"output_tokens\": out_tok,\n",
    "    }\n",
    "\n",
    "@weave.op()\n",
    "def refinement_pass(question: str, draft_answer: str, model: str, temperature: float, \n",
    "                    top_k: int, ppx: float, token_lps: list, topk_alts: list, tokens: list):\n",
    "    \"\"\"Refine answer based on uncertainty analysis\"\"\"\n",
    "    t_start = time.perf_counter()\n",
    "    print(f\"  [REFINEMENT] Starting refinement (perplexity was {ppx:.2f})...\")\n",
    "    \n",
    "    analysis = _uncertainty_report(token_lps, topk_alts, tokens=tokens, max_positions=10)\n",
    "    refined_input = (\n",
    "        \"You previously drafted an answer to a difficult question. \"\n",
    "        \"Analysis shows you were uncertain about specific parts of your response.\\n\\n\"\n",
    "        f\"Original Question: {question}\\n\\n\"\n",
    "        f\"Your Draft Answer:\\n{draft_answer}\\n\\n\"\n",
    "        f\"Detailed Uncertainty Analysis (perplexity={ppx:.3f}):\\n{analysis}\\n\\n\"\n",
    "        \"REFINEMENT INSTRUCTIONS:\\n\"\n",
    "        \"1. Review the uncertain tokens and their alternatives\\n\"\n",
    "        \"2. Consider if any alternatives would be more accurate\\n\"\n",
    "        \"3. Pay special attention to tokens with <50% confidence\\n\"\n",
    "        \"4. For high-entropy tokens, choose the most factually accurate option\\n\"\n",
    "        \"5. Maintain the same structure but improve uncertain parts\\n\\n\"\n",
    "        \"Provide a refined answer that resolves these uncertainties. \"\n",
    "        \"Do not mention this analysis process in your response.\"\n",
    "    )\n",
    "    \n",
    "    print(f\"  [REFINEMENT] Input length: {len(refined_input)} chars\")\n",
    "    \n",
    "    refine_params = {\n",
    "        \"model\": model,\n",
    "        \"instructions\": \"You are a precise cryptography expert. Be concise and accurate.\",\n",
    "        \"input\": refined_input,\n",
    "        \"temperature\": max(0.0, temperature - 0.1),\n",
    "        \"top_logprobs\": top_k,\n",
    "        \"include\": [\"message.output_text.logprobs\"]\n",
    "    }\n",
    "    \n",
    "    print(f\"  [REFINEMENT] Calling OpenAI API...\")\n",
    "    api_start = time.perf_counter()\n",
    "    resp = client.responses.create(**refine_params)\n",
    "    api_end = time.perf_counter()\n",
    "    print(f\"  [REFINEMENT] API call took {api_end - api_start:.2f}s\")\n",
    "    \n",
    "    print(f\"  [REFINEMENT] Extracting metrics...\")\n",
    "    text, token_lps, topk_alts, tokens = _extract_text_and_logprobs(resp)\n",
    "    in_tok, out_tok = _extract_usage(resp)\n",
    "    ppx = _perplexity(token_lps)\n",
    "    avg_lp = (sum(token_lps) / len(token_lps)) if token_lps else None\n",
    "    table = _uncertainty_table(token_lps, topk_alts, max_positions=5)\n",
    "    \n",
    "    t_end = time.perf_counter()\n",
    "    print(f\"  [REFINEMENT] Total time: {t_end - t_start:.2f}s (new perplexity: {ppx:.2f})\")\n",
    "    \n",
    "    return {\n",
    "        \"response\": resp,\n",
    "        \"text\": text,\n",
    "        \"tokens\": tokens,  # NEW: actual token text\n",
    "        \"token_logprobs\": token_lps,\n",
    "        \"perplexity\": ppx,\n",
    "        \"avg_logprob\": avg_lp,\n",
    "        \"uncertainty_table\": table,\n",
    "        \"input_tokens\": in_tok,\n",
    "        \"output_tokens\": out_tok,\n",
    "        \"uncertainty_analysis\": analysis,\n",
    "    }\n",
    "\n",
    "@weave.op()\n",
    "def answer_difficult_question_with_uncertainty(\n",
    "    question: str,\n",
    "    model: str = \"gpt-4.1-mini\",\n",
    "    top_k: int = 5,\n",
    "    threshold: float = 1.4,\n",
    "    temperature: float = 0.2,\n",
    "):\n",
    "    t0 = time.perf_counter()\n",
    "    print(f\"\\n[MAIN] Starting uncertainty-aware generation for: '{question[:50]}...'\")\n",
    "    print(f\"[MAIN] Model: {model}, Threshold: {threshold}\")\n",
    "    \n",
    "    # Reasoning models (o1, o4) don't support temperature or logprobs\n",
    "    is_reasoning_model = model.startswith(('o1', 'o4'))\n",
    "    \n",
    "    # First pass generation\n",
    "    print(f\"[MAIN] Starting first pass...\")\n",
    "    first_pass = first_pass_generation(question, model, temperature, top_k, is_reasoning_model)\n",
    "    print(f\"[MAIN] First pass complete.\")\n",
    "    resp1 = first_pass[\"response\"]\n",
    "\n",
    "    text1 = first_pass[\"text\"]\n",
    "    tokens1 = first_pass.get(\"tokens\", [])  # Extract tokens\n",
    "    token_lps1 = first_pass[\"token_logprobs\"]\n",
    "    topk1 = first_pass[\"top_k_alternatives\"]\n",
    "    in_tok1 = first_pass[\"input_tokens\"]\n",
    "    out_tok1 = first_pass[\"output_tokens\"]\n",
    "    ppx1 = first_pass[\"perplexity\"]\n",
    "    avg_lp1 = first_pass[\"avg_logprob\"]\n",
    "    table1 = first_pass[\"uncertainty_table\"]\n",
    "\n",
    "    did_refine = False\n",
    "    final_text = text1\n",
    "    refinement_data = None\n",
    "    ppx2 = None\n",
    "    avg_lp2 = None\n",
    "    table2 = None\n",
    "    in_tok2, out_tok2 = None, None\n",
    "\n",
    "    # Calculate additional uncertainty metrics for better decision\n",
    "    max_entropy = 0.0\n",
    "    high_uncertainty_tokens = 0\n",
    "    if token_lps1 and topk1:\n",
    "        for i, (lp, alts) in enumerate(zip(token_lps1, topk1)):\n",
    "            entropy = _calculate_entropy(alts)\n",
    "            max_entropy = max(max_entropy, entropy)\n",
    "            # Count tokens with <50% confidence\n",
    "            if math.exp(lp) < 0.5:\n",
    "                high_uncertainty_tokens += 1\n",
    "    \n",
    "    # Skip refinement for reasoning models (no logprobs available)\n",
    "    print(f\"[MAIN] Checking if refinement needed...\")\n",
    "    print(f\"  - Perplexity: {ppx1:.2f} (threshold: {threshold})\")\n",
    "    print(f\"  - Max entropy: {max_entropy:.2f}\")\n",
    "    print(f\"  - High uncertainty tokens: {high_uncertainty_tokens}\")\n",
    "    \n",
    "    # Refine if ANY uncertainty metric is high\n",
    "    should_refine = (\n",
    "        (ppx1 > threshold) or \n",
    "        (max_entropy > 1.5) or  # High entropy = multiple viable options\n",
    "        (high_uncertainty_tokens >= 3)  # Multiple uncertain tokens\n",
    "    )\n",
    "    \n",
    "    if should_refine and not is_reasoning_model:\n",
    "        print(f\"[MAIN] Refinement triggered! Starting refinement pass...\")\n",
    "        did_refine = True\n",
    "        refinement_data = refinement_pass(\n",
    "            question, text1, model, temperature, top_k, ppx1, token_lps1, topk1, tokens1\n",
    "        )\n",
    "        \n",
    "        text2 = refinement_data[\"text\"]\n",
    "        ppx2 = refinement_data[\"perplexity\"]\n",
    "        avg_lp2 = refinement_data[\"avg_logprob\"]\n",
    "        final_text = text2\n",
    "        table2 = refinement_data[\"uncertainty_table\"]\n",
    "        in_tok2 = refinement_data[\"input_tokens\"]\n",
    "        out_tok2 = refinement_data[\"output_tokens\"]\n",
    "        print(f\"[MAIN] Refinement complete.\")\n",
    "    else:\n",
    "        print(f\"[MAIN] No refinement needed (ppx={ppx1:.2f} <= {threshold} or reasoning model)\")\n",
    "    \n",
    "    t1 = time.perf_counter()\n",
    "    print(f\"[MAIN] TOTAL TIME: {t1 - t0:.2f}s\")\n",
    "\n",
    "    # Attach a concise summary to the Weave call for easy inspection in the UI\n",
    "    try:\n",
    "        current_call = weave.require_current_call()\n",
    "        current_call.summary = {\n",
    "            \"parameters\": {\n",
    "                \"model\": model,\n",
    "                \"top_k\": top_k,\n",
    "                \"threshold\": threshold,\n",
    "                \"temperature\": temperature,\n",
    "            },\n",
    "            \"first_pass\": {\n",
    "                \"token_count\": len(token_lps1),\n",
    "                \"avg_logprob\": avg_lp1,\n",
    "                \"perplexity\": ppx1,\n",
    "                \"max_entropy\": max_entropy,\n",
    "                \"high_uncertainty_tokens\": high_uncertainty_tokens,\n",
    "            },\n",
    "            \"refinement\": {\n",
    "                \"enabled\": did_refine,\n",
    "                \"triggered_by\": \"perplexity\" if ppx1 > threshold else (\"entropy\" if max_entropy > 1.5 else \"uncertain_tokens\"),\n",
    "                \"avg_logprob_after\": avg_lp2,\n",
    "                \"perplexity_after\": ppx2,\n",
    "            },\n",
    "            \"timing_seconds\": t1 - t0,\n",
    "        }\n",
    "    except Exception:\n",
    "        # If not in a tracked context, just skip summary attachment\n",
    "        pass\n",
    "\n",
    "    # Aggregate token usage/costs\n",
    "    total_input_tokens = (in_tok1 or 0) + (in_tok2 or 0)\n",
    "    total_output_tokens = (out_tok1 or 0) + (out_tok2 or 0)\n",
    "    estimated_cost_usd = _estimate_cost_usd(model, total_input_tokens, total_output_tokens)\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"final_answer\": final_text,\n",
    "        \"first_pass\": {\n",
    "            \"answer\": text1,\n",
    "            \"avg_logprob\": avg_lp1,\n",
    "            \"perplexity\": ppx1,\n",
    "            \"max_entropy\": max_entropy,\n",
    "            \"high_uncertainty_tokens\": high_uncertainty_tokens,\n",
    "            \"uncertainty_table\": table1,\n",
    "            \"input_tokens\": in_tok1,\n",
    "            \"output_tokens\": out_tok1,\n",
    "        },\n",
    "        \"refinement\": {\n",
    "            \"enabled\": did_refine,\n",
    "            \"triggered_by\": \"perplexity\" if did_refine and ppx1 > threshold else (\"entropy\" if did_refine and max_entropy > 1.5 else (\"uncertain_tokens\" if did_refine else None)),\n",
    "            \"perplexity_after\": ppx2,\n",
    "            \"avg_logprob_after\": avg_lp2,\n",
    "            \"uncertainty_table_after\": table2,\n",
    "            \"input_tokens_after\": in_tok2,\n",
    "            \"output_tokens_after\": out_tok2,\n",
    "        },\n",
    "        \"parameters\": {\n",
    "            \"model\": model,\n",
    "            \"top_k\": top_k,\n",
    "            \"threshold\": threshold,\n",
    "            \"temperature\": temperature,\n",
    "        },\n",
    "        \"usage\": {\n",
    "            \"total_input_tokens\": total_input_tokens if (in_tok1 is not None or in_tok2 is not None) else None,\n",
    "            \"total_output_tokens\": total_output_tokens if (out_tok1 is not None or out_tok2 is not None) else None,\n",
    "            \"estimated_cost_usd\": estimated_cost_usd,\n",
    "            \"timing_seconds\": t1 - t0,\n",
    "        },\n",
    "        \"model_kind\": \"reasoning\" if is_reasoning_model else \"non_reasoning\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acadf5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a difficult question and log everything to Weave via the op above\n",
    "question = (\n",
    "    \"What are the implications of P vs NP for modern cryptography? Provide concrete examples and caveats.\"\n",
    ")\n",
    "with weave.attributes({\n",
    "    \"tag\": \"uncertainty-loop\",\n",
    "    \"question\": question,\n",
    "    \"question_topic\": \"cryptography\",\n",
    "    \"variant\": \"gpt-4.1-mini\",\n",
    "}):\n",
    "    base_result = answer_difficult_question_with_uncertainty(\n",
    "        question,\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        top_k=5,\n",
    "        threshold=1.4,\n",
    "        temperature=0.2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5098220f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "with weave.attributes({\n",
    "    \"tag\": \"uncertainty-loop\",\n",
    "    \"question\": question,\n",
    "    \"question_topic\": \"cryptography\",\n",
    "    \"variant\": \"o4-mini\",\n",
    "}):\n",
    "    reasoning_result = answer_difficult_question_with_uncertainty(\n",
    "        question,\n",
    "        model=\"o4-mini\",\n",
    "        top_k=5,\n",
    "        threshold=1.4,\n",
    "        temperature=0.2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a199d114",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def _fmt_cost(x):\n",
    "    return f\"${x:.4f}\" if isinstance(x, (int, float)) and x is not None else \"-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfbf08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n==== Final Answers ====\")\n",
    "print(\"[gpt-4.1-mini]\\n\", base_result.get(\"final_answer\", \"\"))\n",
    "print(\"\\n[o4-mini]\\n\", reasoning_result.get(\"final_answer\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb749380",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n==== Usage/Cost/Time (estimates) ====\")\n",
    "base_usage = base_result.get(\"usage\", {})\n",
    "reason_usage = reasoning_result.get(\"usage\", {})\n",
    "print(\n",
    "    \"gpt-4.1-mini:\",\n",
    "    \"tokens(in/out)=\",\n",
    "    (base_usage.get(\"total_input_tokens\"), base_usage.get(\"total_output_tokens\")),\n",
    "    \"cost=\",\n",
    "    _fmt_cost(base_usage.get(\"estimated_cost_usd\")),\n",
    "    \"time(s)=\",\n",
    "    base_usage.get(\"timing_seconds\"),\n",
    ")\n",
    "print(\n",
    "    \"o4-mini:\",\n",
    "    \"tokens(in/out)=\",\n",
    "    (reason_usage.get(\"total_input_tokens\"), reason_usage.get(\"total_output_tokens\")),\n",
    "    \"cost=\",\n",
    "    _fmt_cost(reason_usage.get(\"estimated_cost_usd\")),\n",
    "    \"time(s)=\",\n",
    "    reason_usage.get(\"timing_seconds\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4da9f6",
   "metadata": {},
   "source": [
    "## End\n",
    "\n",
    "This notebook now focuses on the Weave-logged uncertainty-aware generation loop using the OpenAI Responses API.\n",
    "Use the Weave UI links to explore traces, inputs/outputs, and compare iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50edd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    # Check if running as a script (not in notebook)\n",
    "    if 'ipykernel' not in sys.modules:\n",
    "        print(\"Running uncertainty-aware generation analysis...\\n\")\n",
    "        \n",
    "        # Test with different types of questions\n",
    "        test_questions = [\n",
    "            \"Is artificial general intelligence likely to be achieved by 2030?\",  # Controversial prediction\n",
    "            \"What are the ethical implications of human genetic enhancement?\",  # Complex ethical question\n",
    "            \"Should cryptocurrency replace traditional banking systems?\",  # Controversial opinion\n",
    "        ]\n",
    "        \n",
    "        for question in test_questions:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print('='*60)\n",
    "            \n",
    "            # Run with non-reasoning model (uncertainty loop)\n",
    "            print(\"\\n▶ GPT-4.1-mini with uncertainty loop:\")\n",
    "            base_result = answer_difficult_question_with_uncertainty(\n",
    "                question,\n",
    "                model=\"gpt-4.1-mini\",\n",
    "                top_k=5,\n",
    "                threshold=1.4,\n",
    "                temperature=0.2,\n",
    "            )\n",
    "            \n",
    "            print(f\"  Answer: {base_result.get('final_answer', '')}\")\n",
    "            print(f\"  Perplexity: {base_result['first_pass'].get('perplexity', 'N/A'):.3f}\")\n",
    "            print(f\"  Refinement: {'✓ Triggered' if base_result['refinement']['enabled'] else '✗ Not needed'}\")\n",
    "            print(f\"  Cost: ${base_result['usage'].get('estimated_cost_usd', 0):.4f}\")\n",
    "            \n",
    "            # Run with reasoning model for comparison\n",
    "            print(\"\\n▶ o4-mini (reasoning model):\")\n",
    "            reasoning_result = answer_difficult_question_with_uncertainty(\n",
    "                question,\n",
    "                model=\"o4-mini\",\n",
    "                top_k=5,\n",
    "                threshold=1.4,\n",
    "                temperature=0.2,\n",
    "            )\n",
    "            \n",
    "            print(f\"  Answer: {reasoning_result.get('final_answer', '')}\")\n",
    "            print(f\"  Cost: ${reasoning_result['usage'].get('estimated_cost_usd', 0):.4f}\")\n",
    "            \n",
    "            # Compare efficiency\n",
    "            cost_ratio = base_result['usage'].get('estimated_cost_usd', 0) / reasoning_result['usage'].get('estimated_cost_usd', 1)\n",
    "            print(f\"\\n  💰 Cost efficiency: {cost_ratio:.1%} of reasoning model cost\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"✅ Check Weave UI for detailed analysis of logprobs and uncertainty metrics!\")\n",
    "        print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
